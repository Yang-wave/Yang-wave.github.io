---
layout: post
title: 强化学习基础
categories: Research Notes
tags: [AI+EDA LAMDA]
---

## 概念

强化学习是用来实现**序贯决策**的机器学习方法。

决策和预测任务不同，决策往往会带来“后果”，因此决策者需要为未来负责，在未来的时间点做出进一步的决策。

### 智能体

代表做决策的机器。

在每一轮交互中，智能体感知到环境目前所处的状态，经过自身的计算给出本轮的动作，将其作用到环境中；环境得到智能体的动作后，产生相应的即时奖励信号并发生相应的状态转移。智能体则在下一轮交互中感知到新的环境状态，依次类推。

**三要素：感知、决策和奖励**

- 感知：智能体在某种程度上感知环境的状态，从而知道自己所处的现状。例如，下围棋的智能体感知当前的棋盘情况。
- 决策：智能体根据当前的状态计算出达到目标需要采取的动作的过程叫作决策。例如，针对当前的棋盘决定下一颗落子的位置。
- 奖励：环境根据状态和智能体采取的动作，产生一个标量信号作为奖励反馈。这个标量信号衡量智能体这一轮动作的好坏。例如，围棋博弈是否胜利；无人车是否安全、平稳且快速地行驶。

因为决策任务是多轮的，智能体就需要在每轮做决策时考虑未来环境相应的改变，**所以当前轮带来最大奖励反馈的动作，在长期来看并不一定是最优的。**

### 环境

环境是动态变化的，即”演变“，是一个随机过程。

对于一个随机过程，其最关键的要素就是状态以及状态转移的条件概率分布。这就好比一个微粒在水中的布朗运动可以由它的起始位置以及下一刻的位置相对当前位置的条件概率分布来刻画。

将智能体的动作因素考虑进去，**那么环境的下一刻状态的概率分布将由当前状态和智能体的动作来共同决定：**
$$
P_{next} \backsim P(·|P_{curr},Action_{Agent} )
$$

### 目标

由于环境的演变是随机过程，所以我们关注回报的期望，定义为**价值**，这就是强化学习中智能体学习的优化目标。

### 强化学习的数据

有监督学习和强化学习在数据层面有很大区别：

* 有监督学习的任务建立在从给定的数据分布中采样得到的训练数据集上，通过优化在训练数据集中设定的目标函数（如最小化预测误差）来找到模型的最优参数。这里，训练数据集背后的数据分布是完全不变的。
* 在强化学习中，数据是在智能体与环境交互的过程中得到的。如果智能体不采取某个决策动作，那么该动作对应的数据就永远无法被观测到，所以**当前智能体的训练数据来自之前智能体的决策结果。**因此，智能体的策略不同，与环境交互所产生的数据分布就不同。

强化学习中有一个关于数据分布的概念，叫作占用度量（occupancy measure）。

归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的**状态动作对（state-action pair）**的概率分布。

占用度量有一个很重要的性质：给定两个策略及其与一个动态环境交互得到的两个占用度量，那么当且仅当这两个占用度量相同时，这两个策略相同。也就是说，如果一个智能体的策略有所改变，那么它和环境交互得到的占用度量也会相应改变。

根据占用度量这一重要的性质，我们可以领悟到强化学习本质的思维方式。

- 强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。因此，强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。
- 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。

### 小结

强化学习任务的最终优化目标是最大化智能体策略在和动态环境**交互过程中的价值**。

策略的价值可以等价转换成奖励函数在策略的占用度量上的期望：
$$
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{equation}
  最优策略 = \argmax_{策略} \ \mathbb{E}(状态，动作) \backsim 
策略的占用度量[奖励函数(状态，动作)]
\end{equation}
$$
有监督学习直接通过优化模型对于数据特征的输出来优化目标，即修改目标函数而数据分布不变；强化学习则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，**即修改数据分布而目标函数不变。**

所以：

**强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。**

## 多臂老虎机问题

### 关键词

* 试错型学习
* 探索与利用

### 问题定义

在多臂老虎机（multi-armed bandit，MAB）问题（见图 2-1）中，有一个拥有K根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布R。

我们每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布中获得一个奖励 。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作T次拉杆后获得尽可能高的累积奖励。

由于奖励的概率分布是未知的，因此我们需要在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”中进行权衡。“采用怎样的操作策略才能使获得的累积奖励最高”便是多臂老虎机问题。

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/640.8908144b.png)

### 要素形式化描述

多臂老虎机问题可以表示为一个元组$\langle \mathcal{A},\mathcal{R} \rangle$，其中

* $\mathcal{A}$是动作集合，包含K个元素（总共有K个老虎机）
* $\mathcal{R}$是奖励概率分布，也就是说每个老虎机的产出对应一个奖励概率分布（而不是固定的）。

多臂老虎机的目标是最大化一段时间步T内累计的奖励。
$$
max \sum^{T}_{t=1}r_{t},r_{t} \backsim \mathcal{R}(·|a_{t})
$$

### 累计懊悔

对于某个动作a，存在一个期望奖励，即其概率分布均值。那么存在一个期望最大的拉杆，表示为$Q^{*}$。

为了更加直观、方便地观察拉动一根拉杆的期望奖励离最优拉杆期望奖励的差距，我们引入**懊悔**（regret）概念。懊悔定义为拉动当前拉杆的动作a与最优拉杆的期望奖励差，即$R(a)=Q^{*}-Q(a)$。累计懊悔为$\sigma_{R}=\sum^{T}_{t=1}R(a_{t})$，即一次完整的T步决策完成后累计的懊悔总量。

MAB问题的目标为最大化累积奖励，等价于最小化累积懊悔。

### 估计期望奖励

由于只拉动一次拉杆获得的奖励存在随机性，所以需要多次拉动一根拉杆，然后计算得到的多次奖励的期望。（**增量式更新**）

### 平衡探索与利用：算法实现

**基本算法框架**

```Python
class Solver:
    """ 多臂老虎机算法基本框架 """
    def __init__(self, bandit):
        self.bandit = bandit
        self.counts = np.zeros(self.bandit.K)  # 每根拉杆的尝试次数
        self.regret = 0.  # 当前步的累积懊悔
        self.actions = []  # 维护一个列表,记录每一步的动作
        self.regrets = []  # 维护一个列表,记录每一步的累积懊悔

    def update_regret(self, k):
        # 计算累积懊悔并保存,k为本次动作选择的拉杆的编号
        self.regret += self.bandit.best_prob - self.bandit.probs[k]
        self.regrets.append(self.regret)

    def run_one_step(self):
        # 返回当前动作选择哪一根拉杆,由每个具体的策略实现
        raise NotImplementedError

    def run(self, num_steps):
        # 运行一定次数,num_steps为总运行次数
        for _ in range(num_steps):
            k = self.run_one_step()
            self.counts[k] += 1
            self.actions.append(k)
            self.update_regret(k)
```

在多臂老虎机问题中，一个经典的问题就是探索与利用的平衡问题。

**探索**（exploration）是指尝试拉动更多可能的拉杆，这根拉杆不一定会获得最大的奖励，但这种方案能够摸清楚所有拉杆的获奖情况。例如，对于一个10臂老虎机，我们要把所有的拉杆都拉动一下才知道哪根拉杆可能获得最大的奖励。

**利用**（exploitation）是指拉动已知期望奖励最大的那根拉杆，由于已知的信息仅仅来自有限次的交互观测，所以当前的最优拉杆不一定是全局最优的。

于是在多臂老虎机问题中，设计策略时就需要平衡探索和利用的次数，使得累积奖励最大化。一个比较常用的思路是在开始时做比较多的探索，在对每根拉杆都有比较准确的估计后，再进行利用。目前已有一些比较经典的算法来解决这个问题，例如上置信界算法和汤普森采样算法等，接下来将分别介绍这几种算法。

#### $\epsilon$-贪心算法

在原来的完全贪婪算法上添加噪声，每次以$1-\epsilon$概率选择目前为止奖励最大的那根拉杆（**利用**），$\epsilon$概率随机选择一根拉杆（**探索**）。

随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，此时我们就没必要继续花大力气进行探索。所以在$\epsilon$-贪婪算法的具体实现中，我们可以令$\epsilon$随时间衰减，即探索的概率将会不断降低。

但是$\epsilon$不会在有限时间内衰减至0。因为基于**有限步数**观测的完全贪婪算法仍然是一个局部信息的贪婪算法，永远距离最优解有一个固定的差距。

固定$\epsilon$的懊悔：

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/output_2_4_2.4825c3f7.png)

$\epsilon$与时间呈反比的懊悔：

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/output_2_4_3.dd308d2c.png)

后者明显更优。

#### 上置信界算法

* 第一根拉杆只被拉动过一次，得到的奖励为0。
* 第二根拉杆被拉动过很多次，我们对它的奖励分布已经有了大致的把握。

问题是要不要继续尝试第一根。

我们可以使用一种基于不确定性的策略来综合考虑现有的期望奖励估值和不确定性，其核心问题是如何估计不确定性。

**上置信界**（upper confidence bound，UCB）算法是一种经典的基于不确定性的策略算法，它的思想用到了一个非常著名的数学原理：**霍夫丁不等式**（Hoeffding's inequality）

在概率论中，霍夫丁不等式给出了随机变量的和与其期望值偏差的概率上限，该不等式被Wassily Hoeffding于1963年提出并证明。霍夫丁不等式是Azuma-Hoeffding不等式的特例。

令$X_1，\dots，X_n$为独立的随机变量，且$X_i\in[a,b],i=1，\dots，n$。这些随机变量的经验均值可表示为：$\bar{X}=\frac{X_1+\dots+X_n}{n}\\$。

霍夫丁不等式叙述如下：
$$
\forall{t>0}，\quad P(\bar{X}-E[\bar{X}]\ge t)\le exp(-\frac{2n^2t^2}{\begin{matrix} \sum_{i=1}^n (b_i-a_i)^2 \end{matrix}})\\
$$
对于伯努利分布来说随机变量的上下界是1和0，所以：
$$
\forall{t>0}，\quad P(\bar{X}-E[\bar{X}]\ge t)\le e^{-2nt^{2}}\\
$$
其中$t=\hat{U_{t}}(a)$代表不确定性度量。给定一个概率$p=e^{-2N_{t}(a)U_{t}(a)^{2}}$，根据不等式（反向）可以得到$Q_{t}(a)<\hat{Q_{t}}(a)+\hat{U_{t}}(a)$至少以概率$1-p$成立。 当p很小的时候$Q_{t}(a)<\hat{Q_{t}}(a)+\hat{U_{t}}(a)$就以很大概率成立。所以$\hat{Q_{t}}(a)+\hat{U_{t}}(a)$便是**期望奖励的上界**。

这里给定p之后，就可以解得$\hat{U_{t}}(a)=\sqrt{\frac{-log \ p}{2N_{t}(a)}}$。

总结：事先设定一个概率p。UCB算法在每次选择拉杆前，先估计每根拉杆的期望奖励的上界，使得拉动每根拉杆的期望奖励只有一个较小的概率p超过这个上界，接着选出期望奖励上界最大的拉杆，从而选择最有可能获得最大期望奖励的拉杆。

实现：设置$p=\frac{1}{t}$，为拉动每根拉杆的次数+1（防止分母为0），并设定一个系数c来控制不确定性的比重。

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/output_2_5.53e4e9e1.png)

（似乎一般，不过没关系）

#### 汤普森采样算法

假设拉动每根拉杆的奖励服从一个特定的概率分布，然后根据拉动每根拉杆的期望奖励来进行选择。

但是由于计算所有拉杆的期望奖励的代价比较高，汤普森采样算法使用采样的方式，即根据当前每个动作a的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。可以看出，汤普森采样是一种计算所有拉杆的最高奖励概率的蒙特卡洛采样方法。

在实际情况中，我们通常用 Beta 分布对当前每个动作的奖励概率分布进行建模。具体来说，若某拉杆被选择k次，其中$m_{1}$次奖励为1，$m_{2}$次奖励为0，则该拉杆的奖励服从参数为$(m_{1}+1,m_{2}+1)$的 Beta分布。

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/641.c16a3dee.png)

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/output_2_6.45dec2db.png)

### 总结

 $\epsilon$-贪婪算法的累积懊悔是随时间线性增长的，而另外 3 种算法（ $\epsilon$-衰减贪婪算法、上置信界算法、汤普森采样算法）的累积懊悔都是随时间次线性增长的（具体为对数形式增长）。

多臂老虎机的每次交互的结果和以往的动作无关，所以可看作**无状态的强化学习**（stateless reinforcement learning）。

## 马尔可夫决策过程

与多臂老虎机问题不同，**马尔可夫决策过程包含状态信息以及状态之间的转移机制。**

当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有**马尔可夫性质**（Markov property）。

我们用二元组$\langle \mathcal{S},\mathcal{P} \rangle$描述马尔可夫过程。前者是有限状态空间，后者是状态转移矩阵。

在马尔可夫过程的基础上加入奖励函数r和折扣因子γ，就可以得到**马尔可夫奖励过程**（Markov reward process），这是一个四元组，多出来的两个元素含义为：

* r是奖励函数，某个状态s的奖励$r(s)$指转移到该状态时可以获得奖励的期望。
* γ是折扣因子（discount factor），γ的取值范围为[0,1)。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的γ更关注长期的累计奖励，接近 0 的γ更考虑短期奖励。

### 回报

在一个马尔可夫奖励过程中，从第t时刻状态$S_{t}$开始（**实际上是给定了一个初始出发状态**），直到终止状态时，考虑衰减的所有奖励之和被称为回报$G_{t}$（Return）。
$$
G_{t}=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k}
$$
这里是无限步。实际中取有限步。

### 价值函数

一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的**价值**（value）。

所有状态的价值就组成了**价值函数**（value function）。这是一个有限集合的满射。
$$
V(s)=\mathbb{E}[G_{t}|S_{t}=s]=\mathbb{E}[R_{t}+\gamma V(S_{t+1})|S_{t}=s]
$$
拆解这个期望，前者是即时奖励的期望（就是该状态的奖励），后者是当前为s的条件下下一状态的奖励期望，即
$$
V(s)=r(s)+\gamma \sum_{s^{'} \in S}p(s^{'}|s)V(s^{'})
$$
此即**贝尔曼方程**（Bellman equation）。

扩展到所有状态：
$$
\mathcal{V}=\mathcal{R}+\gamma \mathcal{PV}
$$
可以直接根据矩阵运算求解（解析解）：
$$
\mathcal{V}=\mathcal{R}+\gamma \mathcal{PV} \\
(I-\gamma \mathcal{P})\mathcal{V}=\mathcal{R} \\
\mathcal{V}=(I-\gamma \mathcal{P})^{-1}\mathcal{R}
$$
计算复杂度$O(n^{3})$，适用于小规模马尔可夫奖励过程。

求解较大规模的马尔可夫奖励过程中的价值函数时，可以使用**动态规划**（dynamic programming）算法、**蒙特卡洛方法**（Monte-Carlo method）和**时序差分**（temporal difference）。

### 马尔可夫决策过程

在MRP上加入动作，就得到了马尔可夫决策过程（MDP）。MDP由五元组$\langle \mathcal{S},\mathcal{A},\mathcal{P},r,\gamma\rangle$构成。

注意：

* $r(s,a)$是奖励函数，同时取决于状态s和动作a
* $P(s^{'}|s,a)$是状态转移函数，表示在状态s执行动作a之后到达状态$s^{'}$的概率。所以$\mathcal{P}$变成三维的了。
* **MDP本身是不包含策略$\pi$的**

智能体与环境MDP的交互示意图：

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/rl-process.723b4a67.png)

### 策略

智能体的策略用$\pi$表示，$\pi(a|s)=P(A_{t}=a|S_{t}=s)$是一个函数，表示在输入状态s情况下采取动作a的概率。

* **确定性策略**（deterministic policy）：在每个状态时只输出一个确定性的动作，即只有该动作的概率为 1，其他动作的概率为 0。
* **随机性策略**（stochastic policy）：在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作。

### 状态价值函数

基于策略$\pi$的状态价值函数（state-value function），定义为从状态s出发遵循策略$\pi$能获得的期望回报，数学表达为：
$$
V^{\pi}(s)=\mathbb{E_{\pi}}[G_{t}|S_{t}=s]
$$

### 动作价值函数

在 MDP 中，由于动作的存在，我们额外定义一个**动作价值函数**（action-value function）。

$Q^{\pi}(s,a)$表示在MDP遵循策略$\pi$时，对当前状态s执行动作a得到的期望回报：
$$
Q^{\pi}(s,a)=\mathbb{E}_{\pi}[G_{t}|S_{t}=s,A_{t}=a]
$$
显然有：
$$
V^{\pi}(s)=\sum_{a \in A}\pi(a|s)Q^{\pi}(s,a)
$$
使用策略$\pi$时，状态s下采取动作s的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：
$$
Q^{\pi}(s,a)=r(s,a)+\gamma \sum_{s^{'} \in S}p(s^{'}|s,a)V^{\pi}(s^{'})
$$

**AI如何控制agent玩游戏？两条路：可以学习策略$\pi$或者学习动作价值函数$Q^{\pi}(s,a)$。**

### 贝尔曼期望方程

通过简单推导就可以分别得到两个价值函数的**贝尔曼期望方程**（Bellman Expectation Equation）:
$$
V^{\pi}(s)=\mathbb{E_{\pi}}[R_{t}+\gamma V^{\pi}(S_{t+1})|S_{t}=s] \\
=\sum_{a \in A}\pi(a|s)(r(s,a)+\gamma \sum_{s^{'} \in S}p(s^{'}|s,a)V^{\pi}(s^{'}))
$$

$$
Q^{\pi}(s,a)=\mathbb{E_{\pi}}[R_{t}+\gamma Q^{\pi}(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a] \\
=r(s,a)+\gamma \sum_{s^{'} \in S}p(s^{'}|s,a)\sum_{a^{'} \in A}\pi(a^{'}|s^{'}Q^{\pi}(s^{'},a^{'}))
$$

![img](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/mdp.aaacb46a.png)

**注意，这时候奖励就取决于在状态s下采取的动作a了**。

例如，为：

* 状态转移函数的一条表示："s1-保持s1-s1": 1.0（**形式为当前状态-采取的动作-下一步到达的状态：到达这个状态的概率**）
* 奖励函数的一条表示："s1-保持s1": -1（**当前状态s和动作a共同决定的Reward**）
* 策略的一条表示："s4-前往s5": 0.5（**当前状态-采取的动作：采取这个动作的概率**）

#### 从MDP到MRP

我们注意到（到目前为止）MDP和MRP的定义方式是不同的，那么我们考虑给定一个MDP和一个策略$\pi$，是否可以将其转化为一个MRP？

答案是肯定的。我们可以将策略的动作选择进行**边缘化**（marginalization)，就可以得到没有动作的 MRP了。具体来说，对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个MRP在该状态下的奖励，即：
$$
r^{'}(s)=\sum_{a \in A}\pi(a|s)r(s,a)
$$
**也就是说，我们当前状态和执行动作共同决定的奖励通过概率加权的方式归到了状态本身。（积分积掉了）**

同理，在MDP中从s转移到下一个状态$s^{'}$的概率也可以通过概率加权的方式消除动作这个因素：
$$
P^{'}(s^{'}|s)=\sum_{a \in \mathcal{A}}\pi (a|s)P(s^{'}|s,a)
$$
根据价值函数的定义可以发现，转化前的 MDP 的状态价值函数和转化后的 MRP 的价值函数是一样的。于是我们可以用 MRP 中计算价值函数的解析解来计算这个 MDP 中该策略的状态价值函数。

### 蒙特卡洛方法

**蒙特卡洛方法**（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。

简单理解：用蒙特卡洛方法估计圆的面积

![用蒙特卡洛方法估计圆的面积](https://repo-for-md.oss-cn-beijing.aliyuncs.com/img/mc.c89f09b0.png)

如何用蒙特卡洛方法来估计一个策略在一个马尔可夫决策过程中的状态价值函数？

回忆一下，在某个策略$\pi$下，一个状态的价值是它的期望回报，那么一个很直观的想法就是用策略$\pi$在 MDP上采样很多条序列，计算从这个状态出发的回报再求其期望就可以了，公式如下：
$$
V^{\pi}(s)=\mathbb{E_{\pi}}[G_{t}|S_{t}=s] \thickapprox \frac{1}{N}\sum_{i=1}^{N}G_{t}^{(i)}
$$
这里介绍的蒙特卡洛价值估计方法会**在该状态每一次出现**时计算它的回报。

（还有一种选择是一条序列只计算一次回报，也就是这条序列第一次出现该状态时计算后面的累积奖励，而后面再次出现该状态时，该状态就被忽略了。）

我们为每一个状态维护一个计数器和总回报，计算状态价值的具体过程如下所示。

* 使用策略$\pi$采样若干条序列
* 对每一条序列中的每一时间步t的状态s进行以下操作：
  * 更新**状态s的计数器**$N(s) \leftarrow N(s)+1$
  * 更新**状态s的总回报**$M(s) \leftarrow M(s)+G_{t}$
  * 可以采用增量更新。
* 每一个状态的价值被估计为回报的平均值$V_{s}=\frac{M(s)}{N(s)}$

根据大数定律，当$N(s) \rightarrow \infty$，有$V(s) \rightarrow V^{\pi}(s)$。

这样可以估计得到**MDP的状态价值**，和MRP解析解得到的状态价值很接近。

### 占用度量

因为对于同一个MDP，不同策略会访问到的状态的概率分布是不同的，所以不同策略的价值函数是不一样的。

定义MDP的初始状态分布（即最初的分布概率）为$v_{0}(s)$，用$P_{t}^{\pi}(s)$来表示采取策略$\pi$使得智能体在t状态时刻为s的概率，所以由$P_{0}^{\pi}(s)=v_{0}(s)$，然后可以定义一个策略的状态访问分布：
$$
v^{\pi}(s)=(a-\gamma)\sum_{t=0}^{\infty}\gamma^{t}P_{t}^{\pi}(s)
$$
**状态访问概率表示一个策略和 MDP 交互会访问到的状态的分布。**（即经过无穷步后访问到各个状态的频率统计）

需要注意的是，理论上在计算该分布时需要交互到无穷步之后，但实际上智能体和 MDP 的交互在一个序列中是有限的。

不过我们仍然可以用以上公式来表达状态访问概率的思想，状态访问概率有如下性质：
$$
v^{\pi}(s')=(1-\gamma)v_{0}(s')+\gamma \int P(s'|s,a)\pi(a|s)v^{\pi}(s)dsda
$$
这个公式的直观理解是：一个状态的访问概率，等于初始的访问概率加上未来可能的所有状态的访问概率的期望，其中每个未来状态的访问概率被其转移概率和策略的概率加权。未来的访问概率是被折扣的，折扣因子γ表示了对未来访问的偏好程度，γ越接近1，表示越看重未来的访问。

**（这是什么？看不太懂）**

然后可以定义**策略的占用度量**：
$$
\rho^{\pi}(s,a)=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}P_{t}^{\pi}(s)\pi(a|s)
$$
它表示动作状态对$(s,a)$被访问到的概率。

这个公式的含义是：在策略π下，状态-动作对(s,a)被访问的概率等于（1-γ）乘以从时间步0到无穷大的，状态s在时间步t被访问的概率乘以在状态s下选择动作a的概率，然后再乘以γ的t次方的和。这个公式考虑了所有可能的时间步，γ的t次方是对未来的折扣。

二者有关系：
$$
\rho^{\pi}(s,a)=v^{\pi}(s)\pi(a|s)
$$
由此可以推出两个重要定理：

* 由于$v^{\pi}(s)$与策略无关，只与状态转移有关，所以
  $$
  \rho^{\pi_{1}}=\rho^{\pi_{2}} \iff \pi_{1}=\pi_{2}
  $$

* 给定一个合法（可能存在的）占用度量$\rho$，可生成该占用度量的唯一策略是
  $$
  \pi_{\rho}=\frac{\rho (s,a)}{\sum_{a'}\rho(s,a')}
  $$
  

### 最优策略

强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。

我们首先定义策略之间的偏序关系：当且仅当对于任意的状态s都有$V^{\pi}(s) \ge V^{\pi '(s)}$，有$\pi \ge \pi '$。所以**在Pareto意义上**至少有一个最优策略（可能有很多个），都表示为$\pi^{*}(s)$。然后可以定义：

* 最优状态价值函数，表示为
  $$
  V^{*}(s)=\max_{\pi}V^{\pi}(s)
  $$

* 最优动作价值函数，表示为
  $$
  Q^{*}(s,a)=\max_{\pi}Q^{\pi}(s,a) \ \ \  \forall s \in S,a \in \mathcal{A}
  $$

为了使$Q^{\pi}(s,a)$最大，我们需要在当前的状态动作对$(s,a)$之后都执行最优策略，于是我们得到了最优状态价值函数和最优动作价值函数之间的关系：
$$
Q^{*}(s,a)=r(s,a)+\gamma \sum_{s' \in S}P(s'|s,a)V^{*}(s')
$$
这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值：
$$
V^{*}(s)=\max_{a \in \mathcal{A}}Q^{*}(s,a)
$$

### 贝尔曼最优方程

根据$V^{*}(s)$和$Q^{*}(s,a)$的关系，我们可以得到**贝尔曼最优方程**（Bellman optimality equation）：
$$
V^{*}(s)=\max_{a \in \mathcal{A}}\{r(s,a)+\gamma \sum_{s' \in \mathcal{S}}p(s'|s,a)V^{*}(s') \} \\
Q^{*}(s,a)=r(s,a)+\gamma \sum_{s' \in \mathcal{S}}p(s'|s,a)\max_{a' \in \mathcal{A}}Q^{*}(s',a')
$$

### 小结

本章从零开始介绍了马尔可夫决策过程的基础概念知识，并讲解了**如何通过求解贝尔曼方程得到状态价值的解析解**以及**如何用蒙特卡洛方法估计各个状态的价值**。马尔可夫决策过程是强化学习中的基础概念，强化学习中的环境就是一个马尔可夫决策过程。我们接下来将要介绍的强化学习算法通常都是在求解马尔可夫决策过程中的最优策略。
