## 1. 什么是强化学习

强化学习（英语：Reinforcement learning，简称*RL*）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。强化学习的核心思想是智能体agent在环境environment中学习，根据环境的状态state（或观测到的observation），执行动作action，并根据环境的反馈 reward（奖励）来指导更好的动作。从环境中获取的状态，有时候叫state，有时候叫observation，这两个其实一个代表全局状态，一个代表局部观测值，在多智能体环境里会有差别，当前环境下，可以把这两个概念划等号。
![强化学习逻辑](/up/20220219/1.jpg)

## 2. 强化学习可以做什么

1. 游戏（马里奥、Atari、Alpha Go、星际争霸等）
2. 机器人控制（机械臂、机器人、自动驾驶、四轴飞行器等）
3. 用户交互（推荐、广告、NLP等）
4. 交通（拥堵管理等）
5. 资源调度（物流、带宽、功率等）
6. 金融（投资组合、股票买卖等）
7. 其他

## 3. 强化学习与监督学习的区别

强化学习、监督学习、非监督学习是机器学习里的三个不同的领域，都跟深度学习有交集。监督学习寻找输入到输出之间的映射，比如分类和回归问题；非监督学习主要寻找数据之间的隐藏关系，比如聚类问题；强化学习则需要在与环境的交互中学习和寻找最佳决策方案。**监督学习处理认知问题，强化学习处理决策问题。**

## 4. 强化学习的如何解决问题

强化学习通过不断的试错探索，吸取经验和教训，持续不断的优化策略，从环境中拿到更好的反馈。其主要有两种学习方案：**基于价值(value-based)**、**基于策略(policy-based)**。

## 5. 强化学习的算法和环境

强化学习的经典算法有：**Q-learning、Sarsa、DQN、Policy Gradient、A3C、DDPG、PPO**；环境分类：**离散控制场景（输出动作可数）**、**连续控制场景（输出动作值不可数）**；强化学习经典环境库*GYM*将环境交互接口规范化为：**重置环境reset()**、**交互step()**、**渲染render()**；强化学习框架库*PARL*将强化学习框架抽象为**Model**、**Algorithm**、**Agent**三层，使得强化学习算法的实现和调试更方便和灵活。

## 6. 动手实践——超级马里奥兄弟

<img src='/up/20220219/2.gif' alt='超级马里奥兄弟'>

1. 游戏环境配置

```shell
# 游戏环境依赖
pip install gym_super_mario_bros nes_py 
```

```python
# 游戏设置
# Import the game
import gym_super_mario_bros
# Import the Joypad wrapper
from nes_py.wrappers import JoypadSpace
# Import the SIMPLIFIED controls
from gym_super_mario_bros.actions import SIMPLE_MOVEMENT

# Setup game
env = gym_super_mario_bros.make('SuperMarioBros-v0')
env = JoypadSpace(env, SIMPLE_MOVEMENT)

# Create a flag - restart or not
done = True
# Loop through each frame in the game
for step in range(100000): 
    # Start the game to begin with 
    if done: 
        # Start the gamee
        env.reset()
    # Do random actions
    state, reward, done, info = env.step(env.action_space.sample())
    # Show the game on the screen
    env.render()
# Close the game
env.close()
```

2. 训练环境配置

```shell
# 训练环境设置
pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio===0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html
pip install stable-baselines3[extra]
```

```python
# Import Frame Stacker Wrapper and GrayScaling Wrapper
from gym.wrappers import GrayScaleObservation
# Import Vectorization Wrappers
from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv
# Import Matplotlib to show the impact of frame stacking
from matplotlib import pyplot as plt

# 1. Create the base environment
env = gym_super_mario_bros.make('SuperMarioBros-v0')
# 2. Simplify the controls 
env = JoypadSpace(env, SIMPLE_MOVEMENT)
# 3. Grayscale
env = GrayScaleObservation(env, keep_dim=True)
# 4. Wrap inside the Dummy Environment
env = DummyVecEnv([lambda: env])
# 5. Stack the frames
env = VecFrameStack(env, 4, channels_order='last')

# env reset
state = env.reset()
# step
state, reward, done, info = env.step([5])
# show
plt.figure(figsize=(20,16))
for idx in range(state.shape[3]):
    plt.subplot(1,4,idx+1)
    plt.imshow(state[0][:,:,idx])
plt.show()
```

3. 模型训练

```python
# Import os for file path management
import os 
# Import PPO for algos
from stable_baselines3 import PPO
# Import Base Callback for saving models
from stable_baselines3.common.callbacks import BaseCallback


class TrainAndLoggingCallback(BaseCallback):

    def __init__(self, check_freq, save_path, verbose=1):
        super(TrainAndLoggingCallback, self).__init__(verbose)
        self.check_freq = check_freq
        self.save_path = save_path

    def _init_callback(self):
        if self.save_path is not None:
            os.makedirs(self.save_path, exist_ok=True)

    def _on_step(self):
        if self.n_calls % self.check_freq == 0:
            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))
            self.model.save(model_path)

        return True

CHECKPOINT_DIR = './train/'
LOG_DIR = './logs/'

# Setup model saving callback
callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)

# This is the AI model started
model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=LOG_DIR, learning_rate=0.000001, n_steps=512)

# Train the AI model, this is where the AI model starts to learn
model.learn(total_timesteps=1000000, callback=callback)

model.save('thisisatestmodel')
```

4. 测一下

```python
# Load model
model = PPO.load('./train/best_model_1000000')

# Start the game 
state = env.reset()

# Loop through the game
while True: 
    action, _ = model.predict(state)
    state, reward, done, info = env.step(action)
    env.render()
```
